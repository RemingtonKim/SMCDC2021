{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959c44e9",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "We run link prediction and leadtime prediction heuristic and node2vec baselines in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c230e",
   "metadata": {},
   "source": [
    "## Heuristic Extraction\n",
    "\n",
    "We extract the heuristics for each node pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759e1af",
   "metadata": {},
   "source": [
    "### Positive Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7562723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "edges_cc = pd.read_csv('../data/edges_cc_dates.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc_dates.csv')\n",
    "edges_pp = pd.read_csv('../data/edges_pp_dates.csv')\n",
    "edges_pp['src'] = edges_pp['src'].astype(str)\n",
    "edges_pp['dst'] = edges_pp['dst'].astype(str)\n",
    "valid_concepts = pd.read_csv('../data/valid_concepts.csv')\n",
    "valid_papers = pd.read_csv('../data/valid_papers.csv')\n",
    "\n",
    "# read in positive samples\n",
    "pos = pd.read_csv('../data/sampled_graphs/date_data_pos.csv')\n",
    "pos.sort_values(by=['end_year', 'end_month'], inplace = True)\n",
    "\n",
    "# load sampled graph\n",
    "with open('../data/sampled_undirected.gpickle', 'rb') as handle:\n",
    "    G = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b515d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all nodes/edges not in sampled graph\n",
    "nodes = set(G.nodes())\n",
    "papers = papers[papers['id'].isin(nodes)].reset_index(drop = True)\n",
    "valid_papers = valid_papers[valid_papers['paper'].isin(nodes)].reset_index(drop = True)\n",
    "valid_concepts = valid_concepts[valid_concepts['CUI'].isin(nodes)].reset_index(drop = True)\n",
    "edges_cc = edges_cc[(edges_cc['src'].isin(nodes)) & (edges_cc['dst'].isin(nodes))].reset_index(drop = True)\n",
    "edges_pc = edges_pc[(edges_pc['src'].isin(nodes)) & (edges_pc['dst'].isin(nodes))].reset_index(drop = True)\n",
    "edges_pp = edges_pp[(edges_pp['src'].isin(nodes)) & (edges_pp['dst'].isin(nodes))].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract heuristics at formation date - leadtime for each positive node pair\n",
    "DyG = nx.Graph()\n",
    "DyG.add_nodes_from(valid_concepts['CUI'].values.tolist(), type='concept')\n",
    "\n",
    "list_cc = []\n",
    "\n",
    "papers_index = 0\n",
    "cc_index = 0\n",
    "pc_index = 0\n",
    "pp_index = 0\n",
    "for i in tqdm(range(len(pos.values))):\n",
    "    \n",
    "    row = pos.values[i]\n",
    "    date = (row[6], row[7])\n",
    "        \n",
    "    if(papers_index < len(papers)):\n",
    "        paper = papers.iloc[papers_index]\n",
    "        while (paper['year'], paper['month']) < date:\n",
    "            DyG.add_node(paper['id'], type='paper')\n",
    "            papers_index+=1\n",
    "            if papers_index<len(papers) :\n",
    "                paper = papers.iloc[papers_index]\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    if(cc_index < len(edges_cc)):\n",
    "        cc = edges_cc.iloc[cc_index]\n",
    "        while (cc['year'], cc['month']) < date:\n",
    "            DyG.add_edge(cc['src'], cc['dst'], type='cc')\n",
    "            cc_index+=1\n",
    "            if cc_index<len(edges_cc) :\n",
    "                cc = edges_cc.iloc[cc_index]\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    if(pc_index < len(edges_pc)):\n",
    "        pc = edges_pc.iloc[pc_index]\n",
    "        while (pc['year'], pc['month']) < date:\n",
    "            DyG.add_edge(pc['src'], pc['dst'], type='pc')\n",
    "            pc_index+=1\n",
    "            if pc_index<len(edges_pc) :\n",
    "                pc = edges_pc.iloc[pc_index]\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    if(pp_index < len(edges_pp)):\n",
    "        pp = edges_pp.iloc[pp_index]\n",
    "        while (pp['year'], pp['month']) < date:\n",
    "            DyG.add_edge(pp['src'], pp['dst'], type='pp')\n",
    "            pp_index+=1\n",
    "            if pp_index<len(edges_pp) :\n",
    "                pp = edges_pp.iloc[pp_index]\n",
    "            else:\n",
    "                break\n",
    "          \n",
    "    src = row[0]\n",
    "    dst = row[1]   \n",
    "    \n",
    "    num_neighbors = sum(1 for _ in nx.common_neighbors(DyG, src, dst))\n",
    "    jaccard = list(nx.jaccard_coefficient(DyG, [(src, dst)]))[0][2]\n",
    "    pa = list(nx.preferential_attachment(DyG, [(src, dst)]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(DyG, [(src, dst)]))[0][2]\n",
    "    ra = list(nx.resource_allocation_index(DyG, [(src, dst)]))[0][2]\n",
    "    \n",
    "    list_cc.append((src, dst, row[2], row[3], row[5], row[6], row[7], num_neighbors, jaccard, pa, aa, ra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame\n",
    "cc = pd.DataFrame(list_cc, columns = ['src', 'dst', 'year', 'month', 'future_time', 'end_year', 'end_month', 'num_neighbors', 'jaccard', 'pa', 'aa', 'ra'])\n",
    "\n",
    "# sort by date\n",
    "cc.sort_values(['year', 'month'], inplace = True)\n",
    "cc.reset_index(inplace = True, drop =True)\n",
    "\n",
    "# save\n",
    "cc.to_csv('../data/sampled_baseline_cc.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ddafb",
   "metadata": {},
   "source": [
    "### Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032dfc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "edges_cc = pd.read_csv('../data/edges_cc_dates.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc_dates.csv')\n",
    "edges_pp = pd.read_csv('../data/edges_pp_dates.csv')\n",
    "edges_pp['src'] = edges_pp['src'].astype(str)\n",
    "edges_pp['dst'] = edges_pp['dst'].astype(str)\n",
    "valid_concepts = pd.read_csv('../data/valid_concepts.csv')\n",
    "valid_papers = pd.read_csv('../data/valid_papers.csv')\n",
    "\n",
    "# read in negative samples\n",
    "neg = pd.read_csv('../data/sampled_graphs/date_data_neg.csv')\n",
    "neg.sort_values(by=['end_year', 'end_month'], inplace = True)\n",
    "    \n",
    "with open('../data/sampled_undirected.gpickle', 'rb') as handle:\n",
    "    G = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf95e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all nodes/edges not in sampled graph\n",
    "nodes = set(G.nodes())\n",
    "papers = papers[papers['id'].isin(nodes)].reset_index(drop = True)\n",
    "valid_papers = valid_papers[valid_papers['paper'].isin(nodes)].reset_index(drop = True)\n",
    "valid_concepts = valid_concepts[valid_concepts['CUI'].isin(nodes)].reset_index(drop = True)\n",
    "edges_cc = edges_cc[(edges_cc['src'].isin(nodes)) & (edges_cc['dst'].isin(nodes))].reset_index(drop = True)\n",
    "edges_pc = edges_pc[(edges_pc['src'].isin(nodes)) & (edges_pc['dst'].isin(nodes))].reset_index(drop = True)\n",
    "edges_pp = edges_pp[(edges_pp['src'].isin(nodes)) & (edges_pp['dst'].isin(nodes))].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract heuristics at date - leadtime for each negative node pair\n",
    "\n",
    "DyG = nx.Graph()\n",
    "DyG.add_nodes_from(valid_concepts['CUI'].values.tolist(), type='concept')\n",
    "\n",
    "list_no_cc = []\n",
    "\n",
    "papers_index = 0\n",
    "cc_index = 0\n",
    "pc_index = 0\n",
    "pp_index = 0\n",
    "for i in tqdm(range(len(neg.values))):\n",
    "    \n",
    "    row = neg.values[i]\n",
    "    date = (row[5], row[6])\n",
    "        \n",
    "    if(papers_index < len(papers)):\n",
    "        paper = papers.iloc[papers_index]\n",
    "        while (paper['year'], paper['month']) < date:\n",
    "            DyG.add_node(paper['id'], type='paper')\n",
    "            papers_index+=1\n",
    "            if papers_index<len(papers) :\n",
    "                paper = papers.iloc[papers_index]\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    if(cc_index < len(edges_cc)):\n",
    "        cc = edges_cc.iloc[cc_index]\n",
    "        while (cc['year'], cc['month']) < date:\n",
    "            DyG.add_edge(cc['src'], cc['dst'], type='cc')\n",
    "            cc_index+=1\n",
    "            if cc_index<len(edges_cc) :\n",
    "                cc = edges_cc.iloc[cc_index]\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    if(pc_index < len(edges_pc)):\n",
    "        pc = edges_pc.iloc[pc_index]\n",
    "        while (pc['year'], pc['month']) < date:\n",
    "            DyG.add_edge(pc['src'], pc['dst'], type='pc')\n",
    "            pc_index+=1\n",
    "            if pc_index<len(edges_pc) :\n",
    "                pc = edges_pc.iloc[pc_index]\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    if(pp_index < len(edges_pp)):\n",
    "        pp = edges_pp.iloc[pp_index]\n",
    "        while (pp['year'], pp['month']) < date:\n",
    "            DyG.add_edge(pp['src'], pp['dst'], type='pp')\n",
    "            pp_index+=1\n",
    "            if pp_index<len(edges_pp) :\n",
    "                pp = edges_pp.iloc[pp_index]\n",
    "            else:\n",
    "                break\n",
    "          \n",
    "    src = row[0]\n",
    "    dst = row[1]   \n",
    "    \n",
    "    num_neighbors = sum(1 for _ in nx.common_neighbors(DyG, src, dst))\n",
    "    jaccard = list(nx.jaccard_coefficient(DyG, [(src, dst)]))[0][2]\n",
    "    pa = list(nx.preferential_attachment(DyG, [(src, dst)]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(DyG, [(src, dst)]))[0][2]\n",
    "    ra = list(nx.resource_allocation_index(DyG, [(src, dst)]))[0][2]\n",
    "    \n",
    "    list_no_cc.append((src, dst, row[2], row[3], row[4], row[5], row[6], num_neighbors, jaccard, pa, aa, ra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame\n",
    "no_cc = pd.DataFrame(list_no_cc, columns = ['src', 'dst', 'year', 'month', 'future_time', 'end_year', 'end_month', 'num_neighbors', 'jaccard', 'pa', 'aa', 'ra'])\n",
    "\n",
    "# save\n",
    "no_cc.to_csv('../data/sampled_baseline_no_cc.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c74b2a",
   "metadata": {},
   "source": [
    "## Heuristic Models - Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3448a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1d9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "cc = pd.read_csv('../data/sampled_baseline_cc.csv')\n",
    "no_cc = pd.read_csv('../data/sampled_baseline_no_cc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37439208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train, val, test split. 1/2019 - 7/2020 = train; 8/2020 = val; 9/2020 - 5/2021 = test\n",
    "cc_split_1 = cc[(cc['year']==2019) & (cc['month']==1)].index[0]\n",
    "cc_split_2 = cc[(cc['year']==2020) & (cc['month']==7)].index[-1]+1\n",
    "cc_split_3 = cc[(cc['year']==2020) & (cc['month']==8)].index[-1]+1\n",
    "train_cc = cc[cc_split_1:cc_split_2].reset_index(drop=True)\n",
    "train_no_cc = no_cc[cc_split_1:cc_split_2].reset_index(drop=True)\n",
    "val_cc = cc[cc_split_2:cc_split_3].reset_index(drop=True)\n",
    "val_no_cc = no_cc[cc_split_2:cc_split_3].reset_index(drop=True)\n",
    "test_cc = cc[cc_split_3:].reset_index(drop=True)\n",
    "test_no_cc = no_cc[cc_split_3:].reset_index(drop=True)\n",
    "\n",
    "assert len(train_cc) == len(train_no_cc)\n",
    "assert len(val_cc) == len(val_no_cc)\n",
    "assert len(test_cc) == len(test_no_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cf2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : 0.8232219851051783; validation : 0.07425634772004704; testing : 0.10252166717477462\n"
     ]
    }
   ],
   "source": [
    "total = len(cc[cc_split_1:])\n",
    "print(\"Training : {}; validation : {}; testing : {}\".format(len(train_cc)/total, len(val_cc)/total, len(test_cc)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ad3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search parameters\n",
    "param_grid = ParameterGrid({'C': [1, 0.1, 0.01, 0.001, 0.0001], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'max_iter':[1000], 'verbose':[1], 'random_state':[12345]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3188500",
   "metadata": {},
   "source": [
    "### Common Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee90d100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 13 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 16 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n",
      "[LibLinear]convergence after 12 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 10 epochs took 0 seconds\n",
      "convergence after 10 epochs took 0 seconds\n",
      "[LibLinear]rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 5 epochs took 0 seconds\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 10 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# training, validation, and testing data\n",
    "common_neighbors_train_x = np.concatenate((train_cc['num_neighbors'].values.reshape([-1, 1]), train_no_cc['num_neighbors'].values.reshape([-1, 1])), axis=0)\n",
    "common_neighbors_train_y = np.concatenate((np.ones(len(train_cc)), np.zeros(len(train_no_cc))), axis = 0)\n",
    "common_neighbors_val_x = np.concatenate((val_cc['num_neighbors'].values.reshape([-1, 1]), val_no_cc['num_neighbors'].values.reshape([-1, 1])), axis=0)\n",
    "common_neighbors_val_y = np.concatenate((np.ones(len(val_cc)), np.zeros(len(val_no_cc))), axis = 0)\n",
    "common_neighbors_test_x = np.concatenate((test_cc['num_neighbors'].values.reshape([-1, 1]), test_no_cc['num_neighbors'].values.reshape([-1, 1])), axis=0)\n",
    "common_neighbors_test_y = np.concatenate((np.ones(len(test_cc)), np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "common_neighbors_scaler = StandardScaler()\n",
    "common_neighbors_train_x = common_neighbors_scaler.fit_transform(common_neighbors_train_x)\n",
    "common_neighbors_val_x = common_neighbors_scaler.transform(common_neighbors_val_x)\n",
    "common_neighbors_test_x = common_neighbors_scaler.transform(common_neighbors_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(common_neighbors_train_x))\n",
    "common_neighbors_train_x = common_neighbors_train_x[p_train]\n",
    "common_neighbors_train_y = common_neighbors_train_y[p_train]\n",
    "p_val = np.random.permutation(len(common_neighbors_val_x))\n",
    "common_neighbors_val_x = common_neighbors_val_x[p_val]\n",
    "common_neighbors_val_y = common_neighbors_val_y[p_val]\n",
    "p_test = np.random.permutation(len(common_neighbors_test_x))\n",
    "common_neighbors_test_x = common_neighbors_test_x[p_test]\n",
    "common_neighbors_test_y = common_neighbors_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure F1 score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_f1 = 0\n",
    "for g in param_grid:\n",
    "    common_neighbors_clf = LogisticRegression(**g).fit(common_neighbors_train_x, common_neighbors_train_y)\n",
    "    \n",
    "    predictions = common_neighbors_clf.predict(common_neighbors_val_x)\n",
    "    f1 = metrics.f1_score(common_neighbors_val_y, predictions)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a04822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6682242990654206\n",
      "ROC-AUC : 0.738019632881431\n",
      "Precision : 0.8173076923076923\n",
      "Recall : 0.4333050127442651\n",
      "F1 : 0.5663520266518601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "common_neighbors_clf = LogisticRegression(**best_grid).fit(common_neighbors_train_x, common_neighbors_train_y)\n",
    "\n",
    "predictions = common_neighbors_clf.predict(common_neighbors_test_x)\n",
    "prob_predictions = common_neighbors_clf.predict_proba(common_neighbors_test_x)[:,-1]\n",
    "\n",
    "accuracy = metrics.accuracy_score(common_neighbors_test_y, predictions)\n",
    "roc_score = metrics.roc_auc_score(common_neighbors_test_y, prob_predictions)\n",
    "precision = metrics.precision_score(common_neighbors_test_y, predictions)\n",
    "recall = metrics.recall_score(common_neighbors_test_y, predictions)\n",
    "f1 = metrics.f1_score(common_neighbors_test_y, predictions)\n",
    "print('Accuracy : {}\\nROC-AUC : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}'.format(accuracy, roc_score, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e860b",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93057944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 17 epochs took 0 seconds\n",
      "convergence after 10 epochs took 0 seconds\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 8 epochs took 0 seconds\n",
      "convergence after 4 epochs took 0 seconds\n",
      "[LibLinear]convergence after 17 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 10 epochs took 0 seconds\n",
      "[LibLinear]convergence after 12 epochs took 0 seconds\n",
      "convergence after 10 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 10 epochs took 0 seconds\n",
      "convergence after 9 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "jaccard_train_x = np.concatenate((train_cc['jaccard'].values.reshape([-1, 1]), train_no_cc['jaccard'].values.reshape([-1, 1])), axis=0)\n",
    "jaccard_train_y = np.concatenate((np.ones(len(train_cc)), np.zeros(len(train_no_cc))), axis = 0)\n",
    "jaccard_val_x = np.concatenate((val_cc['jaccard'].values.reshape([-1, 1]), val_no_cc['jaccard'].values.reshape([-1, 1])), axis=0)\n",
    "jaccard_val_y = np.concatenate((np.ones(len(val_cc)), np.zeros(len(val_no_cc))), axis = 0)\n",
    "jaccard_test_x = np.concatenate((test_cc['jaccard'].values.reshape([-1, 1]), test_no_cc['jaccard'].values.reshape([-1, 1])), axis=0)\n",
    "jaccard_test_y = np.concatenate((np.ones(len(test_cc)), np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "jaccard_scaler = StandardScaler()\n",
    "jaccard_train_x = jaccard_scaler.fit_transform(jaccard_train_x)\n",
    "jaccard_val_x = jaccard_scaler.transform(jaccard_val_x)\n",
    "jaccard_test_x = jaccard_scaler.transform(jaccard_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(jaccard_train_x))\n",
    "jaccard_train_x = jaccard_train_x[p_train]\n",
    "jaccard_train_y = jaccard_train_y[p_train]\n",
    "p_val = np.random.permutation(len(jaccard_val_x))\n",
    "jaccard_val_x = jaccard_val_x[p_val]\n",
    "jaccard_val_y = jaccard_val_y[p_val]\n",
    "p_test = np.random.permutation(len(jaccard_test_x))\n",
    "jaccard_test_x = jaccard_test_x[p_test]\n",
    "jaccard_test_y = jaccard_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure F1 score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_f1 = 0\n",
    "for g in param_grid:\n",
    "    jaccard_clf = LogisticRegression(**g).fit(jaccard_train_x, jaccard_train_y)\n",
    "    \n",
    "    predictions = jaccard_clf.predict(jaccard_val_x)\n",
    "    f1 = metrics.f1_score(jaccard_val_y, predictions)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fdef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Accuracy : 0.6879779099405268\n",
      "ROC-AUC : 0.7414966950089112\n",
      "Precision : 0.7983816587997303\n",
      "Recall : 0.5029736618521665\n",
      "F1 : 0.617148814177743\n"
     ]
    }
   ],
   "source": [
    "jaccard_clf = LogisticRegression(**best_grid).fit(jaccard_train_x, jaccard_train_y)\n",
    "\n",
    "predictions = jaccard_clf.predict(jaccard_test_x)\n",
    "prob_predictions = jaccard_clf.predict_proba(jaccard_test_x)[:,-1]\n",
    "\n",
    "accuracy = metrics.accuracy_score(jaccard_test_y, predictions)\n",
    "roc_score = metrics.roc_auc_score(jaccard_test_y, prob_predictions)\n",
    "precision = metrics.precision_score(jaccard_test_y, predictions)\n",
    "recall = metrics.recall_score(jaccard_test_y, predictions)\n",
    "f1 = metrics.f1_score(jaccard_test_y, predictions)\n",
    "print('Accuracy : {}\\nROC-AUC : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}'.format(accuracy, roc_score, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cac0",
   "metadata": {},
   "source": [
    "### Preferential Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae8e30e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 15 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 15 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n",
      "[LibLinear]convergence after 17 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 11 epochs took 0 seconds\n",
      "[LibLinear]convergence after 6 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 12 epochs took 0 seconds\n",
      "convergence after 9 epochs took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "pa_train_x = np.concatenate((train_cc['pa'].values.reshape([-1, 1]), train_no_cc['pa'].values.reshape([-1, 1])), axis=0)\n",
    "pa_train_y = np.concatenate((np.ones(len(train_cc)), np.zeros(len(train_no_cc))), axis = 0)\n",
    "pa_val_x = np.concatenate((val_cc['pa'].values.reshape([-1, 1]), val_no_cc['pa'].values.reshape([-1, 1])), axis=0)\n",
    "pa_val_y = np.concatenate((np.ones(len(val_cc)), np.zeros(len(val_no_cc))), axis = 0)\n",
    "pa_test_x = np.concatenate((test_cc['pa'].values.reshape([-1, 1]), test_no_cc['pa'].values.reshape([-1, 1])), axis=0)\n",
    "pa_test_y = np.concatenate((np.ones(len(test_cc)), np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "pa_scaler = StandardScaler()\n",
    "pa_train_x = pa_scaler.fit_transform(pa_train_x)\n",
    "pa_val_x = pa_scaler.transform(pa_val_x)\n",
    "pa_test_x = pa_scaler.transform(pa_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(pa_train_x))\n",
    "pa_train_x = pa_train_x[p_train]\n",
    "pa_train_y = pa_train_y[p_train]\n",
    "p_val = np.random.permutation(len(pa_val_x))\n",
    "pa_val_x = pa_val_x[p_val]\n",
    "pa_val_y = pa_val_y[p_val]\n",
    "p_test = np.random.permutation(len(pa_test_x))\n",
    "pa_test_x = pa_test_x[p_test]\n",
    "pa_test_y = pa_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure F1 score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_f1 = 0\n",
    "for g in param_grid:\n",
    "    pa_clf = LogisticRegression(**g).fit(pa_train_x, pa_train_y)\n",
    "    \n",
    "    predictions = pa_clf.predict(pa_val_x)\n",
    "    f1 = metrics.f1_score(pa_val_y, predictions)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e4a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5276125743415463\n",
      "ROC-AUC : 0.5468877790041211\n",
      "Precision : 0.5627413127413128\n",
      "Recall : 0.24766355140186916\n",
      "F1 : 0.343952802359882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "pa_clf = LogisticRegression(**best_grid).fit(pa_train_x, pa_train_y)\n",
    "\n",
    "predictions = pa_clf.predict(pa_test_x)\n",
    "prob_predictions = pa_clf.predict_proba(pa_test_x)[:,-1]\n",
    "\n",
    "accuracy = metrics.accuracy_score(pa_test_y, predictions)\n",
    "roc_score = metrics.roc_auc_score(pa_test_y, prob_predictions)\n",
    "precision = metrics.precision_score(pa_test_y, predictions)\n",
    "recall = metrics.recall_score(pa_test_y, predictions)\n",
    "f1 = metrics.f1_score(pa_test_y, predictions)\n",
    "print('Accuracy : {}\\nROC-AUC : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}'.format(accuracy, roc_score, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173fd16",
   "metadata": {},
   "source": [
    "### Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b5b6268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 16 epochs took 0 seconds\n",
      "convergence after 7 epochs took 0 seconds\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 11 epochs took 0 seconds\n",
      "convergence after 7 epochs took 0 seconds\n",
      "[LibLinear]convergence after 14 epochs took 0 seconds\n",
      "convergence after 10 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 8 epochs took 0 seconds\n",
      "convergence after 10 epochs took 0 seconds\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 8 epochs took 0 seconds\n",
      "convergence after 5 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "aa_train_x = np.concatenate((train_cc['aa'].values.reshape([-1, 1]), train_no_cc['aa'].values.reshape([-1, 1])), axis=0)\n",
    "aa_train_y = np.concatenate((np.ones(len(train_cc)), np.zeros(len(train_no_cc))), axis = 0)\n",
    "aa_val_x = np.concatenate((val_cc['aa'].values.reshape([-1, 1]), val_no_cc['aa'].values.reshape([-1, 1])), axis=0)\n",
    "aa_val_y = np.concatenate((np.ones(len(val_cc)), np.zeros(len(val_no_cc))), axis = 0)\n",
    "aa_test_x = np.concatenate((test_cc['aa'].values.reshape([-1, 1]), test_no_cc['aa'].values.reshape([-1, 1])), axis=0)\n",
    "aa_test_y = np.concatenate((np.ones(len(test_cc)), np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "aa_scaler = StandardScaler()\n",
    "aa_train_x = aa_scaler.fit_transform(aa_train_x)\n",
    "aa_val_x = aa_scaler.transform(aa_val_x)\n",
    "aa_test_x = aa_scaler.transform(aa_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(aa_train_x))\n",
    "aa_train_x = aa_train_x[p_train]\n",
    "aa_train_y = aa_train_y[p_train]\n",
    "p_val = np.random.permutation(len(aa_val_x))\n",
    "aa_val_x = aa_val_x[p_val]\n",
    "aa_val_y = aa_val_y[p_val]\n",
    "p_test = np.random.permutation(len(aa_test_x))\n",
    "aa_test_x = aa_test_x[p_test]\n",
    "aa_test_y = aa_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure F1 score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_f1 = 0\n",
    "for g in param_grid:\n",
    "    aa_clf = LogisticRegression(**g).fit(aa_train_x, aa_train_y)\n",
    "    \n",
    "    predictions = aa_clf.predict(aa_val_x)\n",
    "    f1 = metrics.f1_score(aa_val_y, predictions)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93c4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6607901444350043\n",
      "ROC-AUC : 0.7332444134209274\n",
      "Precision : 0.7982663514578409\n",
      "Recall : 0.43033135089209856\n",
      "F1 : 0.5592050786640905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "aa_clf = LogisticRegression(**best_grid).fit(aa_train_x, aa_train_y)\n",
    "\n",
    "predictions = aa_clf.predict(aa_test_x)\n",
    "prob_predictions = aa_clf.predict_proba(aa_test_x)[:,-1]\n",
    "\n",
    "accuracy = metrics.accuracy_score(aa_test_y, predictions)\n",
    "roc_score = metrics.roc_auc_score(aa_test_y, prob_predictions)\n",
    "precision = metrics.precision_score(aa_test_y, predictions)\n",
    "recall = metrics.recall_score(aa_test_y, predictions)\n",
    "f1 = metrics.f1_score(aa_test_y, predictions)\n",
    "print('Accuracy : {}\\nROC-AUC : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}'.format(accuracy, roc_score, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51e2d7",
   "metadata": {},
   "source": [
    "### Resource Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f82d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 18 epochs took 0 seconds\n",
      "convergence after 16 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 17 epochs took 0 seconds\n",
      "convergence after 16 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 13 epochs took 0 seconds\n",
      "convergence after 15 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]convergence after 12 epochs took 0 seconds\n",
      "convergence after 10 epochs took 0 seconds\n",
      "[LibLinear]convergence after 9 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "ra_train_x = np.concatenate((train_cc['ra'].values.reshape([-1, 1]), train_no_cc['ra'].values.reshape([-1, 1])), axis=0)\n",
    "ra_train_y = np.concatenate((np.ones(len(train_cc)), np.zeros(len(train_no_cc))), axis = 0)\n",
    "ra_val_x = np.concatenate((val_cc['ra'].values.reshape([-1, 1]), val_no_cc['ra'].values.reshape([-1, 1])), axis=0)\n",
    "ra_val_y = np.concatenate((np.ones(len(val_cc)), np.zeros(len(val_no_cc))), axis = 0)\n",
    "ra_test_x = np.concatenate((test_cc['ra'].values.reshape([-1, 1]), test_no_cc['ra'].values.reshape([-1, 1])), axis=0)\n",
    "ra_test_y = np.concatenate((np.ones(len(test_cc)), np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "ra_scaler = StandardScaler()\n",
    "ra_train_x = ra_scaler.fit_transform(ra_train_x)\n",
    "ra_val_x = ra_scaler.transform(ra_val_x)\n",
    "ra_test_x = ra_scaler.transform(ra_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(ra_train_x))\n",
    "ra_train_x = ra_train_x[p_train]\n",
    "ra_train_y = ra_train_y[p_train]\n",
    "p_val = np.random.permutation(len(ra_val_x))\n",
    "ra_val_x = ra_val_x[p_val]\n",
    "ra_val_y = ra_val_y[p_val]\n",
    "p_test = np.random.permutation(len(ra_test_x))\n",
    "ra_test_x = ra_test_x[p_test]\n",
    "ra_test_y = ra_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure F1 score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_f1 = 0\n",
    "for g in param_grid:\n",
    "    ra_clf = LogisticRegression(**g).fit(ra_train_x, ra_train_y)\n",
    "    \n",
    "    predictions = ra_clf.predict(ra_val_x)\n",
    "    f1 = metrics.f1_score(ra_val_y, predictions)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac5b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5488530161427357\n",
      "ROC-AUC : 0.7070537034884854\n",
      "Precision : 0.6448362720403022\n",
      "Recall : 0.21750212404418012\n",
      "F1 : 0.32528589580686146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ra_clf = LogisticRegression(**best_grid).fit(ra_train_x, ra_train_y)\n",
    "\n",
    "predictions = ra_clf.predict(ra_test_x)\n",
    "prob_predictions = ra_clf.predict_proba(ra_test_x)[:,-1]\n",
    "\n",
    "accuracy = metrics.accuracy_score(ra_test_y, predictions)\n",
    "roc_score = metrics.roc_auc_score(ra_test_y, prob_predictions)\n",
    "precision = metrics.precision_score(ra_test_y, predictions)\n",
    "recall = metrics.recall_score(ra_test_y, predictions)\n",
    "f1 = metrics.f1_score(ra_test_y, predictions)\n",
    "print('Accuracy : {}\\nROC-AUC : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}'.format(accuracy, roc_score, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781c01c",
   "metadata": {},
   "source": [
    "## Node2Vec - Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd974f16",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5ba9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nodevectors import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "with open('../data/sampled_undirected.gpickle', 'rb') as handle:\n",
    "    G = pickle.load(handle)\n",
    "\n",
    "cc = pd.read_csv('../data/sampled_baseline_cc.csv')\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "no_cc = pd.read_csv('../data/sampled_baseline_no_cc.csv')\n",
    "cc_split = cc[(cc['year']==2020) & (cc['month']==7)].index[-1]+1\n",
    "test_cc = cc[cc_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove testing and validation edges from graph\n",
    "for i in tqdm(test_cc.index):\n",
    "    row = test_cc.loc[i]\n",
    "    src = row['src']\n",
    "    dst = row['dst']\n",
    "    G.remove_edge(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(G.nodes())\n",
    "papers = papers[papers['id'].isin(nodes)].reset_index(drop = True)\n",
    "papers_split = papers[(papers['year']==2020) & (papers['month']==7)].index[-1]+1\n",
    "papers = papers[papers_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c46bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove papers published after July 2020\n",
    "for i in tqdm(papers.index):\n",
    "    row = papers.loc[i]\n",
    "    G.remove_node(row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10830827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Node2Vec and save model\n",
    "g2v = Node2Vec(n_components=128, walklen=80, epochs=10, threads=0)\n",
    "g2v.fit(G)\n",
    "g2v.save('../data/sampled_node2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e9fa3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51918e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in n2v embeddings\n",
    "g2v = Node2Vec.load('../data/sampled_node2vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0216e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "cc = pd.read_csv('../data/sampled_baseline_cc.csv')\n",
    "no_cc = pd.read_csv('../data/sampled_baseline_no_cc.csv')\n",
    "\n",
    "# train, val, test split. 1/2019 - 7/2020 = train; 8/2020 = val; 9/2020 - 5/2021 = test\n",
    "cc_split_1 = cc[(cc['year']==2019) & (cc['month']==1)].index[0]\n",
    "cc_split_2 = cc[(cc['year']==2020) & (cc['month']==7)].index[-1]+1\n",
    "cc_split_3 = cc[(cc['year']==2020) & (cc['month']==8)].index[-1]+1\n",
    "train_cc = cc[cc_split_1:cc_split_2].reset_index(drop=True)\n",
    "train_no_cc = no_cc[cc_split_1:cc_split_2].reset_index(drop=True)\n",
    "val_cc = cc[cc_split_2:cc_split_3].reset_index(drop=True)\n",
    "val_no_cc = no_cc[cc_split_2:cc_split_3].reset_index(drop=True)\n",
    "test_cc = cc[cc_split_3:].reset_index(drop=True)\n",
    "test_no_cc = no_cc[cc_split_3:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a491523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "n2v_train_x = []\n",
    "for row in tqdm(train_cc.values):\n",
    "    n2v_train_x.append(np.multiply(g2v.predict(row[0]), g2v.predict(row[1])))\n",
    "for row in tqdm(train_no_cc.values):\n",
    "    n2v_train_x.append(np.multiply(g2v.predict(row[0]), g2v.predict(row[1])))\n",
    "n2v_train_x = np.array(n2v_train_x)\n",
    "\n",
    "# val data\n",
    "n2v_val_x = []\n",
    "for row in tqdm(val_cc.values):\n",
    "    n2v_val_x.append(np.multiply(g2v.predict(row[0]), g2v.predict(row[1])))\n",
    "for row in tqdm(val_no_cc.values):\n",
    "    n2v_val_x.append(np.multiply(g2v.predict(row[0]), g2v.predict(row[1])))\n",
    "n2v_val_x = np.array(n2v_val_x)\n",
    "\n",
    "# test data\n",
    "n2v_test_x = []\n",
    "for row in tqdm(test_cc.values):\n",
    "    n2v_test_x.append(np.multiply(g2v.predict(row[0]), g2v.predict(row[1])))\n",
    "for row in tqdm(test_no_cc.values):\n",
    "    n2v_test_x.append(np.multiply(g2v.predict(row[0]), g2v.predict(row[1])))\n",
    "n2v_test_x = np.array(n2v_test_x)\n",
    "\n",
    "# scale data\n",
    "n2v_scaler = StandardScaler()\n",
    "n2v_train_x = n2v_scaler.fit_transform(n2v_train_x)\n",
    "n2v_val_x = n2v_scaler.transform(n2v_val_x)\n",
    "n2v_test_x = n2v_scaler.transform(n2v_test_x)\n",
    "\n",
    "# ground truth\n",
    "n2v_train_y = np.concatenate((np.ones(len(train_cc)), np.zeros(len(train_no_cc))), axis = 0)\n",
    "n2v_val_y = np.concatenate((np.ones(len(val_cc)), np.zeros(len(val_no_cc))), axis = 0)\n",
    "n2v_test_y = np.concatenate((np.ones(len(test_cc)), np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# shuffle train and test arrays in unison\n",
    "p_train = np.random.permutation(len(n2v_train_x))\n",
    "n2v_train_x = n2v_train_x[p_train]\n",
    "n2v_train_y = n2v_train_y[p_train]\n",
    "p_val = np.random.permutation(len(n2v_val_x))\n",
    "n2v_val_x = n2v_val_x[p_val]\n",
    "n2v_val_y = n2v_val_y[p_val]\n",
    "p_test = np.random.permutation(len(n2v_test_x))\n",
    "n2v_test_x = n2v_test_x[p_test]\n",
    "n2v_test_y = n2v_test_y[p_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5717b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to PyTorch tensor\n",
    "n2v_train_x = torch.from_numpy(n2v_train_x).type(torch.float32)\n",
    "n2v_train_y = torch.from_numpy(n2v_train_y).type(torch.float32)\n",
    "n2v_val_x = torch.from_numpy(n2v_val_x).type(torch.float32)\n",
    "n2v_val_y = torch.from_numpy(n2v_val_y).type(torch.float32)\n",
    "n2v_test_x = torch.from_numpy(n2v_test_x).type(torch.float32)\n",
    "n2v_test_y = torch.from_numpy(n2v_test_y).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d803a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer vanilla neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "     \n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac678c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58702cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001, weight_decay=1e-6)\n",
    "\n",
    "train_loss_vals = []\n",
    "val_loss_vals = []\n",
    "for epoch in range(EPOCHS):\n",
    "    net.train()\n",
    "    epoch_train_loss = []\n",
    "    for i in tqdm(range(0, len(n2v_train_x), BATCH_SIZE)):\n",
    "        batch_x = n2v_train_x[i:i+BATCH_SIZE].to(device)\n",
    "        batch_y = n2v_train_y[i:i+BATCH_SIZE].to(device).reshape((-1, 1)).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(batch_x)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, batch_y)\n",
    "        epoch_train_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss_vals.append(sum(epoch_train_loss)/len(epoch_train_loss))\n",
    "    \n",
    "    net.eval()\n",
    "    epoch_val_loss = []\n",
    "    for i in tqdm(range(0, len(n2v_val_x), BATCH_SIZE)):\n",
    "        output = net(n2v_val_x[i:i+BATCH_SIZE].to(device)).cpu()\n",
    "        loss = F.binary_cross_entropy(output, n2v_val_y[i:i+BATCH_SIZE].reshape(-1, 1))\n",
    "        \n",
    "        epoch_val_loss.append(loss.item())\n",
    "\n",
    "    val_loss_vals.append(sum(epoch_val_loss)/len(epoch_val_loss))\n",
    "\n",
    "    print(\"Epoch: {}; Train Loss: {}; Val Loss: {}\".format(epoch, train_loss_vals[-1], val_loss_vals[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f842cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train loss\n",
    "plt.plot(np.linspace(1, EPOCHS, EPOCHS).astype(int), train_loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04efde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot val loss\n",
    "plt.plot(np.linspace(1, EPOCHS, EPOCHS).astype(int), val_loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "with torch.no_grad():\n",
    "    prob_predictions = net(n2v_test_x.to(device)).cpu().numpy()\n",
    "    predictions = (prob_predictions>0.5)\n",
    "    accuracy = metrics.accuracy_score(n2v_test_y.numpy(), predictions)\n",
    "    roc_score = metrics.roc_auc_score(n2v_test_y.numpy(), prob_predictions)\n",
    "    precision = metrics.precision_score(n2v_test_y.numpy(), predictions)\n",
    "    recall = metrics.recall_score(n2v_test_y.numpy(), predictions)\n",
    "    f1 = metrics.f1_score(n2v_test_y, predictions)\n",
    "    print('Accuracy : {}\\nROC-AUC : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}'.format(accuracy, roc_score, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679178c",
   "metadata": {},
   "source": [
    "## Heuristics - Leadtime Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fae2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf372f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : 0.8232219851051783; validation : 0.07425634772004704; testing : 0.10252166717477462\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "cc = pd.read_csv('../data/sampled_baseline_cc.csv')\n",
    "no_cc = pd.read_csv('../data/sampled_baseline_no_cc.csv')\n",
    "\n",
    "# train, val, test split. \n",
    "cc_split_1 = cc[(cc['year']==2019) & (cc['month']==1)].index[0]\n",
    "cc_split_2 = cc[(cc['year']==2020) & (cc['month']==7)].index[-1]+1\n",
    "cc_split_3 = cc[(cc['year']==2020) & (cc['month']==8)].index[-1]+1\n",
    "train_cc = cc[cc_split_1:cc_split_2].reset_index(drop=True)\n",
    "train_no_cc = no_cc[cc_split_1:cc_split_2].reset_index(drop=True)\n",
    "val_cc = cc[cc_split_2:cc_split_3].reset_index(drop=True)\n",
    "val_no_cc = no_cc[cc_split_2:cc_split_3].reset_index(drop=True)\n",
    "test_cc = cc[cc_split_3:].reset_index(drop=True)\n",
    "test_no_cc = no_cc[cc_split_3:].reset_index(drop=True)\n",
    "\n",
    "assert len(train_cc) == len(train_no_cc)\n",
    "assert len(val_cc) == len(val_no_cc)\n",
    "assert len(test_cc) == len(test_no_cc)\n",
    "\n",
    "total = len(cc[cc_split_1:])\n",
    "print(\"Training : {}; validation : {}; testing : {}\".format(len(train_cc)/total, len(val_cc)/total, len(test_cc)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63bf5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search parameters\n",
    "param_grid = ParameterGrid({'C': [1, 0.1, 0.01, 0.001, 0.0001], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], 'max_iter':[1000], 'verbose':[1], 'random_state':[12345], 'multi_class':['multinomial']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d70eb9",
   "metadata": {},
   "source": [
    "### Common Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8829b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 0 seconds\n",
      "convergence after 12 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 1 seconds\n",
      "convergence after 12 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 17 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 12 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 11 epochs took 0 seconds\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 12 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "common_neighbors_train_x = np.concatenate((train_cc['num_neighbors'].values.reshape([-1, 1]), train_no_cc['num_neighbors'].values.reshape([-1, 1])), axis=0)\n",
    "common_neighbors_train_y = np.concatenate((train_cc['future_time'].values, np.zeros(len(train_no_cc))), axis = 0)\n",
    "common_neighbors_val_x = np.concatenate((val_cc['num_neighbors'].values.reshape([-1, 1]), val_no_cc['num_neighbors'].values.reshape([-1, 1])), axis=0)\n",
    "common_neighbors_val_y = np.concatenate((val_cc['future_time'].values, np.zeros(len(val_no_cc))), axis = 0)\n",
    "common_neighbors_test_x = np.concatenate((test_cc['num_neighbors'].values.reshape([-1, 1]), test_no_cc['num_neighbors'].values.reshape([-1, 1])), axis=0)\n",
    "common_neighbors_test_y = np.concatenate((test_cc['future_time'].values, np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "common_neighbors_scaler = StandardScaler()\n",
    "common_neighbors_train_x = common_neighbors_scaler.fit_transform(common_neighbors_train_x)\n",
    "common_neighbors_val_x = common_neighbors_scaler.transform(common_neighbors_val_x)\n",
    "common_neighbors_test_x = common_neighbors_scaler.transform(common_neighbors_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(common_neighbors_train_x))\n",
    "common_neighbors_train_x = common_neighbors_train_x[p_train]\n",
    "common_neighbors_train_y = common_neighbors_train_y[p_train]\n",
    "p_val = np.random.permutation(len(common_neighbors_val_x))\n",
    "common_neighbors_val_x = common_neighbors_val_x[p_val]\n",
    "common_neighbors_val_y = common_neighbors_val_y[p_val]\n",
    "p_test = np.random.permutation(len(common_neighbors_test_x))\n",
    "common_neighbors_test_x = common_neighbors_test_x[p_test]\n",
    "common_neighbors_test_y = common_neighbors_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure accuracy score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_acc = 0\n",
    "for g in param_grid:\n",
    "    common_neighbors_clf = LogisticRegression(**g).fit(common_neighbors_train_x, common_neighbors_train_y)\n",
    "    \n",
    "    predictions = common_neighbors_clf.predict_proba(common_neighbors_val_x)\n",
    "    acc = metrics.roc_auc_score(common_neighbors_val_y, predictions, multi_class='ovr')\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_grid = g\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed71160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6191588785046729\n",
      "AUC: 0.5983486185874572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "common_neighbors_clf = LogisticRegression(**best_grid).fit(common_neighbors_train_x, common_neighbors_train_y)\n",
    "\n",
    "predictions = common_neighbors_clf.predict(common_neighbors_test_x)\n",
    "prob_predictions = common_neighbors_clf.predict_proba(common_neighbors_test_x)\n",
    "\n",
    "print('Accuracy: {}\\nAUC: {}'.format(metrics.accuracy_score(common_neighbors_test_y, predictions), metrics.roc_auc_score(common_neighbors_test_y, prob_predictions, multi_class='ovr')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bda7a",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ce8f976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 18 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 16 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 16 epochs took 0 seconds\n",
      "convergence after 11 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 14 epochs took 0 seconds\n",
      "convergence after 11 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 12 epochs took 0 seconds\n",
      "convergence after 12 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "jaccard_train_x = np.concatenate((train_cc['jaccard'].values.reshape([-1, 1]), train_no_cc['jaccard'].values.reshape([-1, 1])), axis=0)\n",
    "jaccard_train_y = np.concatenate((train_cc['future_time'].values, np.zeros(len(train_no_cc))), axis = 0)\n",
    "jaccard_val_x = np.concatenate((val_cc['jaccard'].values.reshape([-1, 1]), val_no_cc['jaccard'].values.reshape([-1, 1])), axis=0)\n",
    "jaccard_val_y = np.concatenate((val_cc['future_time'].values, np.zeros(len(val_no_cc))), axis = 0)\n",
    "jaccard_test_x = np.concatenate((test_cc['jaccard'].values.reshape([-1, 1]), test_no_cc['jaccard'].values.reshape([-1, 1])), axis=0)\n",
    "jaccard_test_y = np.concatenate((test_cc['future_time'].values, np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "jaccard_scaler = StandardScaler()\n",
    "jaccard_train_x = jaccard_scaler.fit_transform(jaccard_train_x)\n",
    "jaccard_val_x = jaccard_scaler.transform(jaccard_val_x)\n",
    "jaccard_test_x = jaccard_scaler.transform(jaccard_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(jaccard_train_x))\n",
    "jaccard_train_x = jaccard_train_x[p_train]\n",
    "jaccard_train_y = jaccard_train_y[p_train]\n",
    "p_val = np.random.permutation(len(jaccard_val_x))\n",
    "jaccard_val_x = jaccard_val_x[p_val]\n",
    "jaccard_val_y = jaccard_val_y[p_val]\n",
    "p_test = np.random.permutation(len(jaccard_test_x))\n",
    "jaccard_test_x = jaccard_test_x[p_test]\n",
    "jaccard_test_y = jaccard_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure accuracy score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_acc = 0\n",
    "for g in param_grid:\n",
    "    jaccard_clf = LogisticRegression(**g).fit(jaccard_train_x, jaccard_train_y)\n",
    "    \n",
    "    predictions = jaccard_clf.predict_proba(jaccard_val_x)\n",
    "    acc = metrics.roc_auc_score(jaccard_val_y, predictions, multi_class='ovr')\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a9c2c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6257434154630417\n",
      "AUC: 0.617586850282337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "jaccard_clf = LogisticRegression(**best_grid).fit(jaccard_train_x, jaccard_train_y)\n",
    "\n",
    "predictions = jaccard_clf.predict(jaccard_test_x)\n",
    "prob_predictions = jaccard_clf.predict_proba(jaccard_test_x)\n",
    "\n",
    "print('Accuracy: {}\\nAUC: {}'.format(metrics.accuracy_score(jaccard_test_y, predictions), metrics.roc_auc_score(jaccard_test_y, prob_predictions, multi_class='ovr')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662591f5",
   "metadata": {},
   "source": [
    "### PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9010201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 30 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 29 epochs took 1 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 26 epochs took 1 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 19 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 15 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "pa_train_x = np.concatenate((train_cc['pa'].values.reshape([-1, 1]), train_no_cc['pa'].values.reshape([-1, 1])), axis=0)\n",
    "pa_train_y = np.concatenate((train_cc['future_time'].values, np.zeros(len(train_no_cc))), axis = 0)\n",
    "pa_val_x = np.concatenate((val_cc['pa'].values.reshape([-1, 1]), val_no_cc['pa'].values.reshape([-1, 1])), axis=0)\n",
    "pa_val_y = np.concatenate((val_cc['future_time'].values, np.zeros(len(val_no_cc))), axis = 0)\n",
    "pa_test_x = np.concatenate((test_cc['pa'].values.reshape([-1, 1]), test_no_cc['pa'].values.reshape([-1, 1])), axis=0)\n",
    "pa_test_y = np.concatenate((test_cc['future_time'].values, np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "pa_scaler = StandardScaler()\n",
    "pa_train_x = pa_scaler.fit_transform(pa_train_x)\n",
    "pa_val_x = pa_scaler.transform(pa_val_x)\n",
    "pa_test_x = pa_scaler.transform(pa_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(pa_train_x))\n",
    "pa_train_x = pa_train_x[p_train]\n",
    "pa_train_y = pa_train_y[p_train]\n",
    "p_val = np.random.permutation(len(pa_val_x))\n",
    "pa_val_x = pa_val_x[p_val]\n",
    "pa_val_y = pa_val_y[p_val]\n",
    "p_test = np.random.permutation(len(pa_test_x))\n",
    "pa_test_x = pa_test_x[p_test]\n",
    "pa_test_y = pa_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure accuracy score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_acc = 0\n",
    "for g in param_grid:\n",
    "    pa_clf = LogisticRegression(**g).fit(pa_train_x, pa_train_y)\n",
    "    \n",
    "    predictions = pa_clf.predict_proba(pa_val_x)\n",
    "    acc = metrics.roc_auc_score(pa_val_y, predictions, multi_class='ovr')\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921265cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 30 epochs took 1 seconds\n",
      "Accuracy: 0.6274426508071368\n",
      "AUC: 0.4864146797563461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "pa_clf = LogisticRegression(**best_grid).fit(pa_train_x, pa_train_y)\n",
    "\n",
    "predictions = pa_clf.predict(pa_test_x)\n",
    "prob_predictions = pa_clf.predict_proba(pa_test_x)\n",
    "\n",
    "print('Accuracy: {}\\nAUC: {}'.format(metrics.accuracy_score(pa_test_y, predictions), metrics.roc_auc_score(pa_test_y, prob_predictions, multi_class='ovr')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581558a7",
   "metadata": {},
   "source": [
    "### AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e3abbbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 19 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 19 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 16 epochs took 1 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 16 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "rescaling...\n",
      "convergence after 13 epochs took 0 seconds\n",
      "convergence after 13 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "aa_train_x = np.concatenate((train_cc['aa'].values.reshape([-1, 1]), train_no_cc['aa'].values.reshape([-1, 1])), axis=0)\n",
    "aa_train_y = np.concatenate((train_cc['future_time'].values, np.zeros(len(train_no_cc))), axis = 0)\n",
    "aa_val_x = np.concatenate((val_cc['aa'].values.reshape([-1, 1]), val_no_cc['aa'].values.reshape([-1, 1])), axis=0)\n",
    "aa_val_y = np.concatenate((val_cc['future_time'].values, np.zeros(len(val_no_cc))), axis = 0)\n",
    "aa_test_x = np.concatenate((test_cc['aa'].values.reshape([-1, 1]), test_no_cc['aa'].values.reshape([-1, 1])), axis=0)\n",
    "aa_test_y = np.concatenate((test_cc['future_time'].values, np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "aa_scaler = StandardScaler()\n",
    "aa_train_x = aa_scaler.fit_transform(aa_train_x)\n",
    "aa_val_x = aa_scaler.transform(aa_val_x)\n",
    "aa_test_x = aa_scaler.transform(aa_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(aa_train_x))\n",
    "aa_train_x = aa_train_x[p_train]\n",
    "aa_train_y = aa_train_y[p_train]\n",
    "p_val = np.random.permutation(len(aa_val_x))\n",
    "aa_val_x = aa_val_x[p_val]\n",
    "aa_val_y = aa_val_y[p_val]\n",
    "p_test = np.random.permutation(len(aa_test_x))\n",
    "aa_test_x = aa_test_x[p_test]\n",
    "aa_test_y = aa_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure accuracy score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_acc = 0\n",
    "for g in param_grid:\n",
    "    aa_clf = LogisticRegression(**g).fit(aa_train_x, aa_train_y)\n",
    "    \n",
    "    predictions = aa_clf.predict_proba(aa_val_x)\n",
    "    acc = metrics.roc_auc_score(aa_val_y, predictions, multi_class='ovr')\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34d682e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6193712829226848\n",
      "AUC: 0.5984496989031858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "aa_clf = LogisticRegression(**best_grid).fit(aa_train_x, aa_train_y)\n",
    "\n",
    "predictions = aa_clf.predict(aa_test_x)\n",
    "prob_predictions = aa_clf.predict_proba(aa_test_x)\n",
    "\n",
    "print('Accuracy: {}\\nAUC: {}'.format(metrics.accuracy_score(aa_test_y, predictions), metrics.roc_auc_score(aa_test_y, prob_predictions, multi_class='ovr')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c2c9c",
   "metadata": {},
   "source": [
    "### Resource Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f140e1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 59 epochs took 0 seconds\n",
      "convergence after 18 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 58 epochs took 0 seconds\n",
      "convergence after 18 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 33 epochs took 0 seconds\n",
      "convergence after 18 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 32 epochs took 0 seconds\n",
      "convergence after 13 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 14 epochs took 0 seconds\n",
      "convergence after 14 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#training, validation, and testing data\n",
    "ra_train_x = np.concatenate((train_cc['ra'].values.reshape([-1, 1]), train_no_cc['ra'].values.reshape([-1, 1])), axis=0)\n",
    "ra_train_y = np.concatenate((train_cc['future_time'].values, np.zeros(len(train_no_cc))), axis = 0)\n",
    "ra_val_x = np.concatenate((val_cc['ra'].values.reshape([-1, 1]), val_no_cc['ra'].values.reshape([-1, 1])), axis=0)\n",
    "ra_val_y = np.concatenate((val_cc['future_time'].values, np.zeros(len(val_no_cc))), axis = 0)\n",
    "ra_test_x = np.concatenate((test_cc['ra'].values.reshape([-1, 1]), test_no_cc['ra'].values.reshape([-1, 1])), axis=0)\n",
    "ra_test_y = np.concatenate((test_cc['future_time'].values, np.zeros(len(test_no_cc))), axis = 0)\n",
    "\n",
    "# scale data\n",
    "ra_scaler = StandardScaler()\n",
    "ra_train_x = ra_scaler.fit_transform(ra_train_x)\n",
    "ra_val_x = ra_scaler.transform(ra_val_x)\n",
    "ra_test_x = ra_scaler.transform(ra_test_x)\n",
    "\n",
    "# shuffle train, val, and test arrays in unison\n",
    "p_train = np.random.permutation(len(ra_train_x))\n",
    "ra_train_x = ra_train_x[p_train]\n",
    "ra_train_y = ra_train_y[p_train]\n",
    "p_val = np.random.permutation(len(ra_val_x))\n",
    "ra_val_x = ra_val_x[p_val]\n",
    "ra_val_y = ra_val_y[p_val]\n",
    "p_test = np.random.permutation(len(ra_test_x))\n",
    "ra_test_x = ra_test_x[p_test]\n",
    "ra_test_y = ra_test_y[p_test]\n",
    "\n",
    "# fit logistic regression and measure accuracy score on validation set, keeping track of the best set of hyperparameters\n",
    "best_grid = None\n",
    "best_acc = 0\n",
    "for g in param_grid:\n",
    "    ra_clf = LogisticRegression(**g).fit(ra_train_x, ra_train_y)\n",
    "    \n",
    "    predictions = ra_clf.predict_proba(ra_val_x)\n",
    "    acc = metrics.roc_auc_score(ra_val_y, predictions, multi_class='ovr')\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ced89f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6248937977909941\n",
      "AUC: 0.5909136332898937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "ra_clf = LogisticRegression(**best_grid).fit(ra_train_x, ra_train_y)\n",
    "\n",
    "predictions = ra_clf.predict(ra_test_x)\n",
    "prob_predictions = ra_clf.predict_proba(ra_test_x)\n",
    "\n",
    "print('Accuracy: {}\\nAUC: {}'.format(metrics.accuracy_score(ra_test_y, predictions), metrics.roc_auc_score(ra_test_y, prob_predictions, multi_class='ovr')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMCDC2021",
   "language": "python",
   "name": "smcdc2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
