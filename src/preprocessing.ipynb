{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde30eb0",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The preprocessing of data occurs in this notebook. Note that in this context, `future_time` equals `leadtime`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63460d2b",
   "metadata": {},
   "source": [
    "## Concept Name Retrieval\n",
    "\n",
    "We map CUIs to their Unified Medical Language System names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5019a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7705618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMLS versions that will be used. \n",
    "UMLS_VERSION = '2020AA'\n",
    "UMLS_VERSION_2 = '2021AA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in given data\n",
    "edges_cc = pd.read_csv('../data/edges_cc.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232689dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique concepts, which will act as nodes\n",
    "temp_concepts = set(edges_pc['dst']).union(set(edges_cc['src']).union(set(edges_cc['dst'])))\n",
    "# drop all values that are not CUIs; in this case, they are nan, 1, and 2.\n",
    "concepts = [concept for concept in temp_concepts if str(concept)[0]=='C']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb134f",
   "metadata": {},
   "source": [
    "Create a file `../UMLS_API_KEY.txt` to store your API key from https://documentation.uts.nlm.nih.gov/rest/authentication.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in API key\n",
    "with open('../UMLS_API_KEY.txt', 'r') as f:\n",
    "    API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e09c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/HHS/uts-rest-api\n",
    "def generate_service_ticket():\n",
    "    \"\"\"Generates single-use service ticket for UMLS REST API\"\"\"\n",
    "    global API_KEY\n",
    "    \n",
    "    # get Ticket-Granting Ticket\n",
    "    headers = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\", \"User-Agent\":\"python\"}\n",
    "    data = {'apikey': API_KEY}\n",
    "    r = requests.post('https://utslogin.nlm.nih.gov/cas/v1/api-key',data=data,headers=headers)\n",
    "    response = fromstring(r.text)\n",
    "    tgt = response.xpath('//form/@action')[0]\n",
    "    \n",
    "    # get Service Ticket\n",
    "    data = {'service': 'http://umlsks.nlm.nih.gov'}\n",
    "    headers = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\", \"User-Agent\":\"python\" }\n",
    "    r = requests.post(tgt,data=data,headers=headers)\n",
    "    st = r.text\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb662ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve names for every CUI\n",
    "cui_names_list = []\n",
    "failed_cui = []\n",
    "count = 0\n",
    "for cui in tqdm(concepts):\n",
    "    try:\n",
    "        data = requests.get('https://uts-ws.nlm.nih.gov/rest/content/{}/CUI/{}?ticket={}'.format(UMLS_VERSION, cui, generate_service_ticket())).json()\n",
    "        cui_names_list.append({'CUI': cui, 'name': data['result']['name']})\n",
    "    except:\n",
    "        try:\n",
    "            data = requests.get('https://uts-ws.nlm.nih.gov/rest/content/{}/CUI/{}?ticket={}'.format(UMLS_VERSION_2, cui, generate_service_ticket())).json()\n",
    "            cui_names_list.append({'CUI': cui, 'name': data['result']['name']})\n",
    "        except:\n",
    "            failed_cui.append(cui)\n",
    "    count+=1\n",
    "    if count%1000 == 0:\n",
    "        print('Length of cui_names_list : {}; length of failed_cui : {}''.format(len(cui_names_list), len(failed_cui)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .csv\n",
    "cui_names = pd.DataFrame(cui_names_list)\n",
    "cui_names.to_csv('../data/cui_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save failed CUIs to manually add to cui_names.csv\n",
    "failed_cui_df = pd.DataFrame(failed_cui, columns=['CUI'])\n",
    "failed_cui_df.to_csv('../data/failed_cui.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9369b",
   "metadata": {},
   "source": [
    "## Clean Data v1\n",
    "\n",
    "We perform the first round of data cleaning by performing the following tasks:\n",
    "1. Self loops are dropped.\n",
    "2. Papers without valid publication dates (i.e., missing year or month) are dropped because we cannot know whether or not they should be training or validation data.\n",
    "3. Paper-concept edges in which the paper does not have a valid date are dropped. This results in a set of concepts that have at least one valid associated paper.\n",
    "4. Paper-paper edges in which either of the nodes do not have a valid date are dropped.\n",
    "5. Using the set of concepts formed in step 3, concept-concept edges in which either of the nodes are not in the set of valid concepts are dropped.\n",
    "6. Duplicate concept-concept and paper-concept edges or edges with nodes in reverse order (i.e., A-B, B-A) are condensed into one edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebac8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be029c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in given data\n",
    "edges_cc = pd.read_csv('../data/edges_cc.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc.csv')\n",
    "edges_pp = pd.read_csv('../data/edges_pp.csv')\n",
    "papers = pd.read_csv('../data/papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop self loops\n",
    "edges_cc = edges_cc[edges_cc['src']!=edges_cc['dst']]\n",
    "edges_pp = edges_pp[edges_pp['src']!=edges_pp['dst']]\n",
    "edges_pc = edges_pc[edges_pc['src']!=edges_pc['dst']]\n",
    "'Starting lengths - edges_cc : {}, edges_pc : {}, edges_pp : {}, papers : {}'.format(len(edges_cc), len(edges_pc), len(edges_pp), len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all papers that do not have both month and year dates\n",
    "papers = papers[(papers['month'].notna()) & (papers['year'].notna())]\n",
    "'After dropping invalid dates - papers : {}'.format(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f188018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort papers by date\n",
    "papers.sort_values(['year', 'month'], inplace = True)\n",
    "papers.reset_index(inplace = True, drop =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of valid papers\n",
    "valid_papers = list(papers['id'])\n",
    "\n",
    "# drop all paper-concept edges where the paper is invalid\n",
    "edges_pc = edges_pc[edges_pc['src'].isin(valid_papers)]\n",
    "\n",
    "# drop all paper-paper edges where the paper is invalid\n",
    "edges_pp = edges_pp.astype(str)\n",
    "edges_pp = edges_pp[(edges_pp['src'].isin(valid_papers)) & (edges_pp['dst'].isin(valid_papers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7efdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of valid concepts\n",
    "valid_concepts = list(set(edges_pc['dst']))\n",
    "\n",
    "# drop all concept-concept edges where concept is invalid due to it not have an associated paper\n",
    "edges_cc = edges_cc[(edges_cc['src'].isin(valid_concepts)) & (edges_cc['dst'].isin(valid_concepts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame\n",
    "df_valid_papers = pd.DataFrame({'paper': valid_papers})\n",
    "df_valid_concepts = pd.DataFrame({'CUI': valid_concepts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique concept-concept edges, merge duplicates\n",
    "unique_edges_cc = pd.DataFrame(set([tuple(sorted(x)) for x in zip(edges_cc['src'], edges_cc['dst'])]), columns=['src', 'dst'])\n",
    "\n",
    "# unique paper-concept edges, merge duplicates\n",
    "unique_edges_pc = pd.DataFrame(set([x for x in zip(edges_pc['src'], edges_pc['dst'])]), columns=['src', 'dst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total papers : {}; total concepts: {}'.format(len(df_valid_papers), len(df_valid_concepts)))\n",
    "print('Ending lengths - edges_cc : {}, edges_pc : {}, edges_pp : {}'.format(len(unique_edges_cc), len(unique_edges_pc), len(edges_pp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98965ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out processed data\n",
    "unique_edges_cc.to_csv('../data/edges_cc_processed.csv', index=False)\n",
    "unique_edges_pc.to_csv('../data/edges_pc_processed.csv', index=False)\n",
    "edges_pp.to_csv('../data/edges_pp_processed.csv', index=False)\n",
    "papers.to_csv('../data/papers_processed.csv', index=False)\n",
    "df_valid_papers.to_csv('../data/valid_papers.csv', index=False)\n",
    "df_valid_concepts.to_csv('../data/valid_concepts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287bead",
   "metadata": {},
   "source": [
    "## Abstract Retrieval \n",
    "\n",
    "We retrieve the abstracts of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4df0e",
   "metadata": {},
   "source": [
    "Download `metadata.csv` from https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge and store it in `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in metadata from CORD 19, which contains abstracts\n",
    "cord_meta = pd.read_csv('../data/metadata.csv')\n",
    "cord_meta.set_index('cord_uid', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e56e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in valid papers\n",
    "valid_papers = list(pd.read_csv('../data/valid_papers.csv')['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850eb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate PubMed and CORD 19 papers. PubMed papers have purely numeric IDs while CORD 19 paper IDs are alphanumeric\n",
    "pubmed = [x for x in valid_papers if x.isnumeric()]\n",
    "s_pubmed = set(pubmed)\n",
    "cord = [x for x in valid_papers if x not in s_pubmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all possible CORD 19 abstracts\n",
    "cord_abstract = []\n",
    "for paper in tqdm(cord):\n",
    "    try:\n",
    "        cord_abstract.append((paper, cord_meta.loc[paper]['abstract']))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c84339",
   "metadata": {},
   "source": [
    "Create a file `../NCBI_API_KEY.txt` to store your API key from https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b47527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in API key\n",
    "with open('../NCBI_API_KEY.txt', 'r') as f:\n",
    "    API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544faf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all possible PubMed abstracts\n",
    "pubmed_abstract = []\n",
    "for paper in tqdm(pubmed):\n",
    "    try:\n",
    "        data = requests.get('http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={}&retmode=XML&rettype=abstract&api_key={}'.format(paper, API_KEY)).text\n",
    "        data = BeautifulSoup(data)\n",
    "        pubmed_abstract.append((paper, data.pubmedarticleset.pubmedarticle.abstract.abstracttext.get_text()))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out retrieved abtracts, dropping any NaN rows\n",
    "pubmed_abstract_df = pd.DataFrame(pubmed_abstract, columns=['paper', 'abstract'])\n",
    "pubmed_abstract_df.dropna(inplace = True)\n",
    "pubmed_abstract_df.to_csv('../data/pubmed_abstracts.csv', index = False)\n",
    "cord_abstract_df = pd.DataFrame(cord_abstract, columns=['paper', 'abstract'])\n",
    "cord_abstract_df.dropna(inplace = True)\n",
    "cord_abstract_df.to_csv('../data/cord_abstracts.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfffb9",
   "metadata": {},
   "source": [
    "## Node Embeddings\n",
    "\n",
    "We create the node embeddings using the names and abstracts we retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in Google News Word2Vec\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1ed8a",
   "metadata": {},
   "source": [
    "### Paper Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in retrieved abtracts\n",
    "cord_abstracts = pd.read_csv('../data/cord_abstracts.csv')\n",
    "pubmed_abstracts = pd.read_csv('../data/pubmed_abstracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af445a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized abstracts for w2v\n",
    "cord_tokenize = []\n",
    "for paper, abstract in tqdm(list(zip(cord_abstracts['paper'], cord_abstracts['abstract']))):\n",
    "    cord_tokenize.append((paper, [word.lower() for word in nltk.word_tokenize(abstract) if word.isalpha()]))\n",
    "pubmed_tokenize = []\n",
    "for paper, abstract in tqdm(list(zip(pubmed_abstracts['paper'], pubmed_abstracts['abstract']))):\n",
    "    pubmed_tokenize.append((paper, [word.lower() for word in nltk.word_tokenize(abstract) if word.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778070b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed abstracts for TF-IDF\n",
    "cord_processed = []\n",
    "for i in tqdm(cord_tokenize):\n",
    "    cord_processed.append(' '.join(i[1]))\n",
    "pubmed_processed = []\n",
    "for i in tqdm(pubmed_tokenize):\n",
    "    pubmed_processed.append(' '.join(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get TF-IDF values\n",
    "vect = TfidfVectorizer()\n",
    "tfidf_matrix = vect.fit_transform(cord_processed+pubmed_processed)\n",
    "tfidf_values = dict(zip(vect.get_feature_names(), vect.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all TF-IDF weighted averages of w2v vectors for each abstract, which will be the node feature for the paper node\n",
    "abstract_embeddings = []\n",
    "count = 0\n",
    "for i in tqdm(cord_tokenize):\n",
    "    paper = i[0]\n",
    "    abstract = i[1]\n",
    "    embeddings = []\n",
    "    used_unk = False\n",
    "    used_not_unk = False\n",
    "    for word in abstract:\n",
    "        try:\n",
    "            word_vector = wv[word]\n",
    "            used_not_unk = True\n",
    "        except:\n",
    "            word_vector = wv['unk']\n",
    "            used_unk = True\n",
    "        try:\n",
    "            word_vector *= tfidf_values[word]\n",
    "        except:\n",
    "            pass\n",
    "        embeddings.append(word_vector)\n",
    "    if(len(embeddings)==0):\n",
    "        embeddings.append(wv['unk'])\n",
    "        count+=1\n",
    "    elif(not used_not_unk and used_unk):\n",
    "        count+=1\n",
    "    abstract_embeddings.append((paper, np.mean(embeddings, axis=0)))\n",
    "\n",
    "for i in tqdm(pubmed_tokenize):\n",
    "    paper = i[0]\n",
    "    abstract = i[1]\n",
    "    embeddings = []\n",
    "    used_unk = False\n",
    "    used_not_unk = False\n",
    "    for word in abstract:\n",
    "        try:\n",
    "            word_vector = wv[word]\n",
    "            used_not_unk = True\n",
    "        except:\n",
    "            word_vector = wv['unk']\n",
    "            used_unk = True\n",
    "        try:\n",
    "            word_vector *= tfidf_values[word]\n",
    "        except:\n",
    "            pass\n",
    "        embeddings.append(word_vector)\n",
    "    if(len(embeddings)==0):\n",
    "        embeddings.append(wv['unk'])\n",
    "        count+=1\n",
    "    elif(not used_not_unk and used_unk):\n",
    "        count+=1\n",
    "    abstract_embeddings.append((paper, np.mean(embeddings, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the number of papers with only unknown characters in their final embedding, not the number of papers with one or more unknown characters\n",
    "'Number of paper nodes with unk : {}'.format(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame and save\n",
    "embeddings = pd.DataFrame(abstract_embeddings, columns=['paper', 'embedding'])\n",
    "embeddings.to_hdf('../data/paper_embeddings.h5', index = False, key='df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c2588",
   "metadata": {},
   "source": [
    "### Concept Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in UMLS names and valid concepts\n",
    "cui_names = pd.read_csv('../data/cui_names.csv')\n",
    "cui_names.set_index('CUI', drop = True, inplace = True)\n",
    "valid_concepts = list(pd.read_csv('../data/valid_concepts.csv')['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized concept names for w2v\n",
    "concept_tokenize = []\n",
    "for concept in tqdm(valid_concepts):\n",
    "    concept_tokenize.append((concept, [x.lower() for x in nltk.word_tokenize(cui_names.loc[concept]['name']) if x.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed concept names for TF-IDF\n",
    "concept_processed = []\n",
    "for i in tqdm(concept_tokenize):\n",
    "    if(len(i[1])>0):\n",
    "        concept_processed.append(' '.join(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1994c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get TF-IDF values\n",
    "vect = TfidfVectorizer()\n",
    "tfidf_matrix = vect.fit_transform(concept_processed)\n",
    "tfidf_values = dict(zip(vect.get_feature_names(), vect.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all TF-IDF weighted averages of w2v vectors for each concept name, which will be the node feature for the concept node\n",
    "concept_embeddings = []\n",
    "count = 0\n",
    "for i in tqdm(concept_tokenize):\n",
    "    concept = i[0]\n",
    "    name = i[1]\n",
    "    embeddings = []\n",
    "    used_unk = False\n",
    "    used_not_unk = False\n",
    "    for word in name:\n",
    "        try:\n",
    "            word_vector = wv[word]\n",
    "            used_not_unk = True\n",
    "        except:\n",
    "            word_vector = wv['unk']\n",
    "            used_unk = True\n",
    "        try:\n",
    "            word_vector *= tfidf_values[word]\n",
    "        except:\n",
    "            pass\n",
    "        embeddings.append(word_vector)\n",
    "    if(len(embeddings)==0):\n",
    "        embeddings.append(wv['unk'])\n",
    "        count+=1\n",
    "    elif(not used_not_unk and used_unk):\n",
    "        count+=1\n",
    "    concept_embeddings.append((concept, np.mean(embeddings, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556aa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the number of concepts with only unknown characters in their final embedding, not the number of concepts with one or more unknown characters\n",
    "'Number of concept nodes with unk : {}'.format(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3483b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame and save\n",
    "embeddings = pd.DataFrame(concept_embeddings, columns=['CUI', 'embedding'])\n",
    "embeddings.to_hdf('../data/concept_embeddings.h5', index = False, key='df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4442506",
   "metadata": {},
   "source": [
    "## Clean Data v2\n",
    "\n",
    "We perform the second round of data cleaning by performing the following tasks:\n",
    "1. Papers without a valid abstract are dropped because they do not have node features.\n",
    "2. Paper-concept and paper-paper edges in which the paper became invalidated due to step 1 are dropped.\n",
    "3. Concept-concept edges in which a concept became invalidated due to its paper being dropped in step 2 are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "edges_cc = pd.read_csv('../data/edges_cc_processed.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc_processed.csv')\n",
    "edges_pp = pd.read_csv('../data/edges_pp_processed.csv')\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "valid_papers = list(pd.read_csv('../data/valid_papers.csv')['paper'])\n",
    "valid_concepts = list(pd.read_csv('../data/valid_concepts.csv')['CUI'])\n",
    "paper_node_features = pd.read_hdf('../data/paper_embeddings.h5')\n",
    "concept_node_features = pd.read_hdf('../data/concept_embeddings.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Starting lengths - edges_cc : {}, edges_pc : {}, edges_pp : {}, valid_papers : {}; valid_concepts: {}'.format(len(edges_cc), len(edges_pc), len(edges_pp), len(valid_papers), len(valid_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ba4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all papers without valid abstracts\n",
    "papers = papers[papers['id'].isin([str(x) for x in paper_node_features['paper']])]\n",
    "papers.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of valid papers\n",
    "valid_papers = list(papers['id'])\n",
    "\n",
    "# drop all paper-concept edges where the paper is invalid\n",
    "edges_pc = edges_pc[edges_pc['src'].isin(valid_papers)]\n",
    "\n",
    "# drop all paper-paper edges where the paper is invalid\n",
    "edges_pp = edges_pp.astype(str)\n",
    "edges_pp = edges_pp[(edges_pp['src'].isin(valid_papers)) & (edges_pp['dst'].isin(valid_papers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad450a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of valid concepts\n",
    "valid_concepts = list(set(edges_pc['dst']))\n",
    "\n",
    "# drop all concept-concept edges where concept is invalid due to it not have an associated paper\n",
    "edges_cc = edges_cc[(edges_cc['src'].isin(valid_concepts)) & (edges_cc['dst'].isin(valid_concepts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ending lengths - edges_cc : {}, edges_pc : {}, edges_pp : {}, valid_papers : {}; valid_concepts: {}'.format(len(edges_cc), len(edges_pc), len(edges_pp), len(valid_papers), len(valid_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame\n",
    "df_valid_papers = pd.DataFrame({'paper': valid_papers})\n",
    "df_valid_concepts = pd.DataFrame({'CUI': valid_concepts})\n",
    "\n",
    "# write out processed data\n",
    "edges_cc.to_csv('../data/edges_cc_processed.csv', index=False)\n",
    "edges_pc.to_csv('../data/edges_pc_processed.csv', index=False)\n",
    "edges_pp.to_csv('../data/edges_pp_processed.csv', index=False)\n",
    "papers.to_csv('../data/papers_processed.csv', index=False)\n",
    "df_valid_papers.to_csv('../data/valid_papers.csv', index=False)\n",
    "df_valid_concepts.to_csv('../data/valid_concepts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac693008",
   "metadata": {},
   "source": [
    "## Date Assignments\n",
    "\n",
    "We assign dates to all the edges and correct dates on the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a35319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c466d33",
   "metadata": {},
   "source": [
    "### CC Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32880b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data for cc edges\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "edges_cc = pd.read_csv('../data/edges_cc_processed.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary where keys are concepts and values are the set of papers associated with the concept\n",
    "pc_dict = {}\n",
    "for i in tqdm(range(len(edges_pc))):\n",
    "    concept = edges_pc.iloc[i]['dst']\n",
    "    paper = edges_pc.iloc[i]['src']\n",
    "    if concept not in pc_dict:\n",
    "        pc_dict[concept] = set()\n",
    "    pc_dict[concept].add(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to paper IDs\n",
    "papers.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all concept-concept edges to find the papers that link them together and assign the date as the earliest paper's date\n",
    "edges_cc_dates_list = []\n",
    "failed_intersection = []\n",
    "for index in tqdm(range(len(edges_cc))):\n",
    "    row = edges_cc.iloc[index]\n",
    "    intersection = list(pc_dict[row['src']].intersection(pc_dict[row['dst']]))\n",
    "    if len(intersection) > 0:\n",
    "        year = papers.loc[intersection[0]]['year']\n",
    "        month = papers.loc[intersection[0]]['month']\n",
    "        for i in range(1, len(intersection)):\n",
    "            cur_year = papers.loc[intersection[i]]['year']\n",
    "            cur_month = papers.loc[intersection[i]]['month']\n",
    "            if cur_year > year:\n",
    "                pass\n",
    "            elif cur_year == year:\n",
    "                month = min(month, cur_month)\n",
    "            else:\n",
    "                year = cur_year\n",
    "                month = cur_month\n",
    "        edges_cc_dates_list.append((row['src'], row['dst'], year, month, len(intersection)))\n",
    "    else:\n",
    "        # there are no papers that link the two concepts, so we cannot assign a date to it\n",
    "        failed_intersection.append((row['src'], row['dst']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d013ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DataFrame\n",
    "edges_cc_dates = pd.DataFrame(edges_cc_dates_list, columns = ['src', 'dst', 'year', 'month', 'num_paper_link'])\n",
    "failed_cc_edges = pd.DataFrame(failed_intersection, columns = ['src', 'dst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Number of final cc edges : {}; number of failed cc edges : {}'.format(len(edges_cc_dates), len(failed_cc_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d458213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "edges_cc_dates.sort_values(['year', 'month'], inplace = True)\n",
    "edges_cc_dates.reset_index(inplace = True, drop =True)\n",
    "\n",
    "# correct dates that are after May 2021\n",
    "edges_cc.loc[(edges_cc['year']==2021) & (edges_cc['month']>5), 'month'] = 5\n",
    "\n",
    "# save\n",
    "edges_cc_dates.to_csv('../data/edges_cc_dates.csv', index = False)\n",
    "failed_cc_edges.to_csv('../data/failed_cc_edges.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86c1ef",
   "metadata": {},
   "source": [
    "### PP Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data for pp edges\n",
    "edges_pp = pd.read_csv('../data/edges_pp_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign dates to each pp edge\n",
    "edges_pp_dates_list = []\n",
    "for index in tqdm(range(len(edges_pp))):\n",
    "    row = edges_pp.iloc[index]\n",
    "    edges_pp_dates_list.append((row['src'], row['dst'], papers.loc[str(row['src'])]['year'], papers.loc[str(row['src'])]['month']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c342361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DataFrame\n",
    "edges_pp_dates = pd.DataFrame(edges_pp_dates_list, columns = ['src', 'dst', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad72815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "edges_pp_dates.sort_values(['year', 'month'], inplace = True)\n",
    "edges_pp_dates.reset_index(inplace = True, drop =True)\n",
    "\n",
    "# correct dates that are after May 2021\n",
    "edges_pp.loc[(edges_pp['year']==2021) & (edges_pp['month']>5), 'month'] = 5\n",
    "\n",
    "# save\n",
    "edges_pp_dates.to_csv('../data/edges_pp_dates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ee3a4",
   "metadata": {},
   "source": [
    "### PC Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data for pc edges\n",
    "edges_pc = pd.read_csv('../data/edges_pc_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign dates to each pc edge\n",
    "edges_pc_dates_list = []\n",
    "for index in tqdm(range(len(edges_pc))):\n",
    "    row = edges_pc.iloc[index]\n",
    "    edges_pc_dates_list.append((row['src'], row['dst'], papers.loc[str(row['src'])]['year'], papers.loc[str(row['src'])]['month']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DataFrame\n",
    "edges_pc_dates = pd.DataFrame(edges_pc_dates_list, columns = ['src', 'dst', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df95c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "edges_pc_dates.sort_values(['year', 'month'], inplace = True)\n",
    "edges_pc_dates.reset_index(inplace = True, drop =True)\n",
    "\n",
    "# correct dates that are after May 2021\n",
    "edges_pc.loc[(edges_pc['year']==2021) & (edges_pc['month']>5), 'month'] = 5\n",
    "\n",
    "# save\n",
    "edges_pc_dates.to_csv('../data/edges_pc_dates.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd23c33",
   "metadata": {},
   "source": [
    "### Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "\n",
    "# correct dates that are after May 2021\n",
    "papers.loc[(papers['year']==2021) & (papers['month']>5), 'month'] = 5\n",
    "\n",
    "# save\n",
    "papers.to_csv('../data/papers_processed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ee2b8",
   "metadata": {},
   "source": [
    "## Network Formation\n",
    "\n",
    "We form a network using the data we preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "edges_cc = pd.read_csv('../data/edges_cc_dates.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc_dates.csv')\n",
    "edges_pp = pd.read_csv('../data/edges_pp_dates.csv')\n",
    "edges_pp['src'] = edges_pp['src'].astype(str)\n",
    "edges_pp['dst'] = edges_pp['dst'].astype(str)\n",
    "valid_concepts = list(pd.read_csv('../data/valid_concepts.csv')['CUI'])\n",
    "valid_papers = list(pd.read_csv('../data/valid_papers.csv')['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e3205",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Number of papers: {}; Number of concepts: {}'.format(len(valid_papers), len(valid_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab52099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph object\n",
    "G = nx.Graph()\n",
    "\n",
    "# add both concept and paper nodes\n",
    "for paper in tqdm(valid_papers):\n",
    "    G.add_node(paper, data = {'type': 'paper'})\n",
    "    \n",
    "for concept in tqdm(valid_concepts):\n",
    "    G.add_node(concept, data = {'type': 'concept'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Number of nodes: {}'.format(G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add concept-concept, paper-concept, and paper-paper edges to G\n",
    "past_edge_num = 0\n",
    "\n",
    "G.add_edges_from(list(zip(edges_cc['src'], edges_cc['dst'])), type='cc')\n",
    "print('Number of cc edges: {}'.format(G.number_of_edges()))\n",
    "past_edge_num = G.number_of_edges()\n",
    "\n",
    "G.add_edges_from(list(zip(edges_pc['src'], edges_pc['dst'])), type='pc')\n",
    "print('Number of pc edges: {}'.format(G.number_of_edges() - past_edge_num))\n",
    "past_edge_num = G.number_of_edges()\n",
    "\n",
    "G.add_edges_from(list(zip(edges_pp['src'], edges_pp['dst'])), type='pp')\n",
    "print('Number of pp edges: {}'.format(G.number_of_edges() - past_edge_num))\n",
    "\n",
    "print('Number of total edges: {}'.format(G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write networkX object to binary\n",
    "nx.readwrite.gpickle.write_gpickle(G, '../data/graph_all_data_undirected.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9306d",
   "metadata": {},
   "source": [
    "## Graph Sampling\n",
    "\n",
    "We utilize the Forest Fire Sampler from [Leskovec & Faloutsos](https://cs.stanford.edu/~jure/pubs/sampling-kdd06.pdf) to sample our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b06e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "from littleballoffur import ForestFireSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in graph\n",
    "with open('../data/graph_all_data_undirected.gpickle', 'rb') as handle:\n",
    "    g = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37dc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the largest connected component, which is only original minus ~6000 nodes\n",
    "cc = nx.connected_components(g)\n",
    "l = sorted(list(cc), key=lambda x: len(x), reverse = True)\n",
    "sub_g = g.subgraph(l[0])\n",
    "sub_g = nx.convert_node_labels_to_integers(sub_g, label_attribute='name')\n",
    "\n",
    "# starting statistics\n",
    "num_nodes = sub_g.number_of_nodes()\n",
    "num_edges = sub_g.number_of_edges()\n",
    "print('Starting number of nodes : {}; number of edges : {}'.format(num_nodes, num_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of nodes to keep\n",
    "PERCENTAGE = 0.15\n",
    "\n",
    "# sample from network\n",
    "sampler = ForestFireSampler(number_of_nodes=num_nodes * PERCENTAGE)\n",
    "sampled = sampler.sample(sub_g)\n",
    "\n",
    "num_nodes = sampled.number_of_nodes()\n",
    "num_edges = sampled.number_of_edges()\n",
    "print('Sampled number of nodes : {}; number of edges : {}'.format(num_nodes, num_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel nodes with names\n",
    "mapping = {row[0]:row[1]['name'] for row in list(sampled.nodes(data = True))}\n",
    "sampled = nx.relabel_nodes(sampled, mapping)\n",
    "\n",
    "# remove name attribute\n",
    "for (n,d) in sampled.nodes(data=True):\n",
    "    del d['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fdec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open('../data/sampled_undirected.gpickle', 'wb') as handle:\n",
    "    pickle.dump(sampled, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b51e29",
   "metadata": {},
   "source": [
    "## networkX to PyTorch Geometric\n",
    "\n",
    "We convert our networkX graph object to a PyTorch Geometric compatible format. We write out a version of the network at every timestep between January 2014 and May 2021 as they will be used to construct the sequences of graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebd579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sampled graph\n",
    "with open('../data/sampled_undirected.gpickle', 'rb') as handle:\n",
    "    g = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "papers = pd.read_csv('../data/papers_processed.csv')\n",
    "edges_cc = pd.read_csv('../data/edges_cc_dates.csv')\n",
    "edges_pc = pd.read_csv('../data/edges_pc_dates.csv')\n",
    "edges_pp = pd.read_csv('../data/edges_pp_dates.csv')\n",
    "edges_pp['src'] = edges_pp['src'].astype(str)\n",
    "edges_pp['dst'] = edges_pp['dst'].astype(str)\n",
    "valid_concepts = pd.read_csv('../data/valid_concepts.csv')\n",
    "valid_papers = pd.read_csv('../data/valid_papers.csv')\n",
    "paper_node_features = pd.read_hdf('../data/paper_embeddings.h5')\n",
    "paper_node_features['paper'] = paper_node_features['paper'].astype(str)\n",
    "paper_node_features.set_index('paper', inplace = True, drop = True)\n",
    "concept_node_features = pd.read_hdf('../data/concept_embeddings.h5')\n",
    "concept_node_features.set_index('CUI', inplace = True, drop = True)\n",
    "node_features = paper_node_features.append(concept_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a72d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(g.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdf48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all edges and nodes not in the sampled graph\n",
    "papers = papers[papers['id'].isin(nodes)].reset_index(drop = True)\n",
    "valid_papers = valid_papers[valid_papers['paper'].isin(nodes)].reset_index(drop = True)\n",
    "valid_concepts = valid_concepts[valid_concepts['CUI'].isin(nodes)].reset_index(drop = True)\n",
    "edges_cc = edges_cc[(edges_cc['src'].isin(nodes)) & (edges_cc['dst'].isin(nodes))].reset_index(drop = True)\n",
    "edges_pc = edges_pc[(edges_pc['src'].isin(nodes)) & (edges_pc['dst'].isin(nodes))].reset_index(drop = True)\n",
    "edges_pp = edges_pp[(edges_pp['src'].isin(nodes)) & (edges_pp['dst'].isin(nodes))].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_val = edges_cc.values\n",
    "pc_val = edges_pc.values\n",
    "pp_val = edges_pp.values\n",
    "node_names = np.concatenate((valid_concepts.values.flatten(), valid_papers.values.flatten()))\n",
    "\n",
    "with open('../data/sampled_graphs/node_names.pickle', 'wb') as handle:\n",
    "    pickle.dump(node_names, handle,  protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign an index to each concept and paper\n",
    "index_dict = {}\n",
    "for i in tqdm(range(len(valid_concepts.values))):\n",
    "    index_dict[valid_concepts.values[i][0]] = i\n",
    "for i in tqdm(range(len(valid_papers.values))):\n",
    "    index_dict[valid_papers.values[i][0]] = i + len(valid_concepts)\n",
    "\n",
    "with open('../data/sampled_graphs/index_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(index_dict, handle,  protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcccee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch compatible format\n",
    "edge_index = [[], []]\n",
    "tensor_edge_index = torch.LongTensor(edge_index)\n",
    "\n",
    "# self loops are added so that the node exists in the graph\n",
    "for row in tqdm(valid_concepts.values):\n",
    "    edge_index[0].append(index_dict[row[0]])\n",
    "    edge_index[1].append(index_dict[row[0]])\n",
    "for row in tqdm(valid_papers.values):\n",
    "    edge_index[0].append(index_dict[row[0]])\n",
    "    edge_index[1].append(index_dict[row[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all edges up to and including 1/2014\n",
    "STOP_DATE = (2014, 1)\n",
    "cc_index = 0\n",
    "pc_index = 0\n",
    "pp_index = 0\n",
    "for row in tqdm(cc_val):\n",
    "    year = row[2]\n",
    "    month = row[3]\n",
    "    if (year, month) > STOP_DATE:\n",
    "        break\n",
    "    else:\n",
    "        edge_index[0].append(index_dict[row[0]])\n",
    "        edge_index[1].append(index_dict[row[1]])\n",
    "        edge_index[0].append(index_dict[row[1]])\n",
    "        edge_index[1].append(index_dict[row[0]])\n",
    "        cc_index+=1\n",
    "        \n",
    "for row in tqdm(pc_val):\n",
    "    year = row[2]\n",
    "    month = row[3]\n",
    "    if (year, month) > STOP_DATE:\n",
    "        break\n",
    "    else:\n",
    "        edge_index[0].append(index_dict[row[0]])\n",
    "        edge_index[1].append(index_dict[row[1]])\n",
    "        edge_index[0].append(index_dict[row[1]])\n",
    "        edge_index[1].append(index_dict[row[0]])\n",
    "        pc_index+=1\n",
    "        \n",
    "for row in tqdm(pp_val):\n",
    "    year = row[2]\n",
    "    month = row[3]\n",
    "    if (year, month) > STOP_DATE:\n",
    "        break\n",
    "    else:\n",
    "        edge_index[0].append(index_dict[row[0]])\n",
    "        edge_index[1].append(index_dict[row[1]])\n",
    "        edge_index[0].append(index_dict[row[1]])\n",
    "        edge_index[1].append(index_dict[row[0]])\n",
    "        pp_index+=1\n",
    "tensor_edge_index = torch.LongTensor(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72846e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform dimensionality reduction on the node features using PCA\n",
    "features = torch.tensor([node_features.loc[node]['embedding']  for node in node_names])\n",
    "pca = PCA(n_components=32)\n",
    "pca.fit(features)\n",
    "features = torch.tensor(pca.transform(features), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sampled_graphs/node_features.pickle', 'wb') as handle:\n",
    "    pickle.dump(features, handle,  protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out every version of the network from January 2014 to May 2021 to be used in the graph sequences\n",
    "date_index = []\n",
    "graph_date = (2014, 1)\n",
    "while graph_date < (2021, 5):\n",
    "    print(\"Graph date : {}\".format(graph_date))\n",
    "    \n",
    "    with open('../data/sampled_graphs/graph_{}_{}.pickle'.format(graph_date[0], graph_date[1]), 'wb') as handle:\n",
    "        pickle.dump(tensor_edge_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    date_index.append(graph_date)\n",
    "    graph_date = (graph_date[0]+graph_date[1]//12, graph_date[1]%12+1)\n",
    "    while(cc_index < len(cc_val)):\n",
    "        row = cc_val[cc_index]\n",
    "        year = row[2]\n",
    "        month = row[3]\n",
    "        if (year, month) <= graph_date:\n",
    "            edge_index[0].append(index_dict[row[0]])\n",
    "            edge_index[1].append(index_dict[row[1]])\n",
    "            edge_index[0].append(index_dict[row[1]])\n",
    "            edge_index[1].append(index_dict[row[0]])\n",
    "            cc_index+=1\n",
    "        else:\n",
    "            break\n",
    "    while(pc_index < len(pc_val)):\n",
    "        row = pc_val[pc_index]\n",
    "        year = row[2]\n",
    "        month = row[3]\n",
    "        if (year, month) <= graph_date:\n",
    "            edge_index[0].append(index_dict[row[0]])\n",
    "            edge_index[1].append(index_dict[row[1]])\n",
    "            edge_index[0].append(index_dict[row[1]])\n",
    "            edge_index[1].append(index_dict[row[0]])\n",
    "            pc_index+=1\n",
    "        else:\n",
    "            break\n",
    "    while(pp_index < len(pp_val)):\n",
    "        row = pp_val[pp_index]\n",
    "        year = row[2]\n",
    "        month = row[3]\n",
    "        if (year, month) <= graph_date:\n",
    "            edge_index[0].append(index_dict[row[0]])\n",
    "            edge_index[1].append(index_dict[row[1]])\n",
    "            edge_index[0].append(index_dict[row[1]])\n",
    "            edge_index[1].append(index_dict[row[0]])\n",
    "            pp_index+=1\n",
    "        else:\n",
    "            break\n",
    "    tensor_edge_index = torch.LongTensor(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out date indices\n",
    "with open('../data/sampled_graphs/date_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(date_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a9822",
   "metadata": {},
   "source": [
    "## Leadtime Assignments\n",
    "\n",
    "We assign leadtime to concept node pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ec5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b815aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sampled graph and date indices\n",
    "with open('../data/sampled_undirected.gpickle', 'rb') as handle:\n",
    "    g = pickle.load(handle)\n",
    "\n",
    "with open('../data/sampled_graphs/date_index.pickle', 'rb') as handle:\n",
    "    date_index = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c9228",
   "metadata": {},
   "source": [
    "### Positive Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and drop all edges/nodes not in the sampled network\n",
    "edges_cc = pd.read_csv('../data/edges_cc_dates.csv')\n",
    "nodes = set(g.nodes())\n",
    "edges_cc = edges_cc[(edges_cc['src'].isin(nodes)) & (edges_cc['dst'].isin(nodes))].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba345db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all edges formed after or in January 2019 are used\n",
    "to_use = edges_cc.iloc[edges_cc[(edges_cc['year']==2018) & (edges_cc['month']==12)].index[-1]+1:]\n",
    "to_use.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e57c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a leadtime. 0 = 1 month (immediate future), 1 = 1 year, 2 = 2 years, 3 = 3 years\n",
    "to_use = to_use.assign(future_time=pd.Series(np.random.randint(0,4, len(to_use))).values)\n",
    "to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what date formation date - leadtime is \n",
    "date_data = []\n",
    "for row in tqdm(to_use.values):\n",
    "    l = list(row)\n",
    "    if(row[5] == 0):\n",
    "        if(row[3] == 1):\n",
    "            l.append(row[2] - 1)\n",
    "            l.append(12)\n",
    "        else:\n",
    "            l.append(row[2])\n",
    "            l.append(row[3] - 1)\n",
    "    else:\n",
    "        l.append(row[2] - row[5])\n",
    "        l.append(row[3])\n",
    "    date_data.append(l)\n",
    "\n",
    "# determine where the starting and ending indices should be\n",
    "BACK = 36\n",
    "for row in date_data:\n",
    "    i = date_index.index((row[-2], row[-1]))\n",
    "    row.append(i-BACK)\n",
    "    row.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame and save\n",
    "date_data = pd.DataFrame(date_data, columns = ['src', 'dst', 'year', 'month', 'num_paper_link', 'future_time', 'end_year', 'end_month', 'start_index', 'end_index'])\n",
    "date_data.to_csv('../data/sampled_graphs/date_data_pos.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dffe23",
   "metadata": {},
   "source": [
    "### Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a748b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sampled graph\n",
    "with open('../data/sampled_undirected.gpickle', 'rb') as handle:\n",
    "    g = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample negative node pairs\n",
    "list_no_cc = set()\n",
    "while len(list_no_cc)<len(date_data):\n",
    "    src = valid_concepts.values[np.random.randint(0, len(valid_concepts))][0]\n",
    "    dst = valid_concepts.values[np.random.randint(0, len(valid_concepts))][0]\n",
    "    if not g.has_edge(src, dst):\n",
    "        list_no_cc.add((src, dst))\n",
    "\n",
    "list_no_cc = list(list_no_cc)\n",
    "no_cc = pd.DataFrame(list_no_cc, columns=['src', 'dst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign date of May 2021 to all node pairs\n",
    "no_cc = no_cc.assign(year=pd.Series([2021.0 for x in range(len(no_cc))]).values)\n",
    "no_cc = no_cc.assign(month=pd.Series([5.0 for x in range(len(no_cc))]).values)\n",
    "\n",
    "# assign a leadtime. 0 = 1 month (immediate future), 1 = 1 year, 2 = 2 years, 3 = 3 years\n",
    "no_cc = no_cc.assign(future_time=pd.Series(np.random.randint(0,4, len(to_use))).values)\n",
    "no_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0829d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what date - leadtime is \n",
    "neg_date_data = []\n",
    "for row in tqdm(no_cc.values):\n",
    "    l = list(row)\n",
    "    if(row[4] == 0):\n",
    "        if(row[3] == 1):\n",
    "            l.append(row[2] - 1)\n",
    "            l.append(12)\n",
    "        else:\n",
    "            l.append(row[2])\n",
    "            l.append(row[3] - 1)\n",
    "    else:\n",
    "        l.append(row[2] - row[4])\n",
    "        l.append(row[3])\n",
    "    neg_date_data.append(l)\n",
    "\n",
    "# determine where the starting and ending indices should be\n",
    "BACK = 36\n",
    "for row in neg_date_data:\n",
    "    i = date_index.index((row[-2], row[-1]))\n",
    "    row.append(i-BACK)\n",
    "    row.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af042c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame and save\n",
    "neg_date_data = pd.DataFrame(neg_date_data, columns = ['src', 'dst', 'year', 'month', 'future_time', 'end_year', 'end_month', 'start_index', 'end_index'])\n",
    "neg_date_data.to_csv('../data/sampled_graphs/date_data_neg.csv', index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMCDC2021",
   "language": "python",
   "name": "smcdc2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
